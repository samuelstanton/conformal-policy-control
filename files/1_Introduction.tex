\section{Introduction}


Consequential decision making often requires navigating a fundamental tension between probable safety on the one hand, and a chance of extraordinary success  on the other. This dilemma has many variations and is known by many names, such as the explore-exploit trade-off in reinforcement learning, the strategies of diversification versus specialization in long-term planning, and the tenets of safety versus efficacy in clinical trials. In statistics, even the tradeoff between validity (i.e., controlling false discoveries or ``Type I'' errors) versus power or efficiency (i.e., maximizing the true detection rate or controlling ``Type II'' errors) has a similar flavor. More broadly, the ongoing debate as to how to balance ensuring AI systems are safe, versus advancing their capabilities to benefit society, urgently demands progress toward principled and practical solutions. 


This tension between AI safety and capabilities becomes even more prominent and challenging as these systems are increasingly endowed with \textit{agency}---the ability to act autonomously, with consequences to human well-being often at stake. In particular, deploying AI agents at scale poses at least three fundamental challenges: (1) Firstly, agents' actions over time may create what we refer to as \textit{feedback-loop shifts}---successive data distributions that are dependent on previous distributions---which can invalidate standard statistical inference.
% (which is often premised on a ``static world'' assumption of distributional invariance). 
Prime examples include experimental design, where an agent's decision about what experiment to run changes the distribution where it receives feedback, and robotics, where an agent chooses new physical environments to explore. (2) Secondly, generative AI agents based on large language models (LLMs) have large, potentially infinite action spaces. For instance, in biomolecular engineering, this might be the space of all possible protein sequences, while for LLM chatbots, it could be the space of possible text responses or chain-of-thought sequences. (3) Lastly, it is important to ensure that any risks being managed align with the values of the human stakeholders. For instance, there has been surging progress in uncertainty and risk \textit{quantification} of AI outputs in recent years, but a question largely remains: how can this methodological progress be translated in real-world practice, to \textit{improve decision making and outcomes}?


% (Decision making, dilemma of safety versus uncertainty)

% (Challenges with AI agents: Feedback loop shifts, large action spaces, risk criteria of interest to human end users)

% (Conformal prediction and conformal risk control)


    
    \begin{table*}[ht]
    \caption{Summary comparison of selected related work on conformal prediction for decision making under agent-induced shifts.}
    \centering
    {\small
    \begin{tabular}{ c | c | c | c  c  c } 
    \toprule
    % \cline{3-4}
    & & &  \multicolumn{3}{|c}{\textbf{3. Type of Risk Control}}  \\ 
     % \cmidrule(lr){2-4}
     % \multirow{2}{*}{2} & Rep-tile: & $\set{2}$ & $\set{3}$ & \\
     % \cline{2-4}
     % &  &  &  &  &  \\
     %%
      $\begin{matrix}
      \textbf{Method Group}\\ 
      \text{References}
      \end{matrix}$ & $\begin{matrix}
        \textbf{1. Agent-} \\
            \textbf{Induced} \\
           \textbf{Data Shift}
       \end{matrix}$
     & $\begin{matrix}
     \textbf{2. Action} \\ 
     \textbf{Space}
     \end{matrix}$ & $\begin{matrix}\textbf{Prediction Set} \\ \textbf{Miscoverage} \\ \mathbbm{1}\{Y_t\not\in \hat{C}_{\color{blue}\alpha}(X_t)\} \end{matrix}$ & $\begin{matrix}\lambda\textbf{-Parameterized} \\ \textbf{Losses} \\ L_t(A_t({\color{blue}\lambda}))\end{matrix}$ & 
      $\begin{matrix}
          \textbf{Unparameterizable} \\
          \textbf{Losses} \\
          \textbf{($\beta$-Param. Policy)} \\
          L_t(A_t), \ A_t\sim p_t({\color{blue}\beta})
      \end{matrix}$
       \\
     \hline
     $\begin{matrix}\textbf{Conformal Prediction} \\ 
     \text{\citet{vovk2005algorithmic}}
     \\
     \text{\citet{vovk2018conformal}} 
     \end{matrix}$ & \xmark & $\begin{matrix}
         \text{Small/} \\ \text{Finite}
     \end{matrix}$ & \cmark & \xmark & \xmark \\
     \hline
     $\begin{matrix}\textbf{Weighted Conformal Pred.} \\ 
     \text{\citet{tibshirani2019conformal}}
     \\
     \text{\citet{fannjiang2022conformal}} \\
     \text{\citet{stanton2023bayesian}} \\
     \text{\citet{nair2023randomization}} \\
     \text{\citet{prinster2024conformal}} \\
     \end{matrix}$  & $\begin{matrix} 
         \text{Single-Round} \\ 
    
         \hline \\
         \textbf{\mygreen{Multi-Round}} 
     \end{matrix}$ & $\begin{matrix}
         \text{Small/} \\ \text{Finite} \\ \hline  \text{Contin.} \\ \hline \text{Small/} \\ \text{Finite}
     \end{matrix}$ & \cmark & \xmark & \xmark \\
     \hline
     $\begin{matrix}\textbf{Conformal Risk Control} \\ 
     \text{\citet{angelopoulos2022conformal}}
     \\
     \text{\citet{lekeufack2024conformal}} 
     \end{matrix}$ & $\begin{matrix}
      \\
         \text{Single-Round} \\ 
         \hline 
         \text{Eventually safe}
     \end{matrix}$ & $\begin{matrix}
         \text{Small/} \\ \text{Finite} 
     \end{matrix}$ & \cmark & \cmark & \xmark \\
     \hline
     $\begin{matrix} \textbf{Conformal Policy Control} \\
     \text{(Proposed)}
     \end{matrix}$ & \textbf{\mygreen{Multi-Round}} & $\begin{matrix}
         \text{Large/} \\ \boldsymbol{\infty}
     \end{matrix}$ & \cmark & \cmark & \cmark  \\
    \bottomrule 
    \end{tabular}
    }
    \label{table:comparisons}
    \end{table*}
    

While decision making under uncertainty has been widely studied across AI/ML, statistics, economics, and other fields, most existing reliability guarantees fail to hold in the case of AI agents. That is, most approaches to statistical uncertainty quantification---a prerequisite to statistical decision theory---rely on restrictive parametric assumptions (which are difficult to verify or enforce), require distributional invariance  (e.g., independent and identically distributed or ``IID'' data) which is violated by feedback-loop shifts, or only provide a weak form of ``long-run'' or ``eventual'' safety. A noteable exception is \textit{conformal prediction} (CP) \citep{vovk2005algorithmic, angelopoulos2024theoretical}, for which standard variants provide nonparametric guarantees \citep{vovk2005algorithmic} and recent ``weighted'' extensions have advanced the ability to handle certain agent-induced feedback-loop shifts \citep{tibshirani2019conformal, fannjiang2022conformal, nair2023randomization, prinster2024conformal}. 


In this work, we build on recent progress in conformal prediction to develop \textit{conformal policy control} (CPC): a practical and statistically-principled framework for enabling black-box AI agents to automatically determine their own ``zone of competence'' based on their available data, where they are able to respect a user-specified risk constraint. Our CPC methods can be viewed as extending conformal risk control (CRC) \citep{angelopoulos2022conformal} to enable controlling risks where one cannot directly parameterize the loss function (as is needed in CRC), but where one can control the \textit{probabilities} or \textit{policy} governing the risk. Our CPC methods address the key challenges posed by AI agents---that is, of feedback-loop shifts, large/infinite action spaces, and flexible, user-specified risk-criteria---and achieve \textit{nonparametric, test-time} reliability guarantees (Table \ref{table:comparisons}). We evaluate our proposed methods on real-world data with a simulated active learning task, as well as on a rigorous biomolecular design benchmark with LLM agents.





% \drew{(Placeholder intro)}

% A machine learning (ML) model is only as good as its data.
% All too often we set out to solve a problem with ML only to find the initial data we were given are insufficient for the task.
% As a result solving a problem with ML entails not only finding the right model, but also constructing the right dataset.
% When relevant data is easy to find constructing a dataset is primarily a matter of aggregation and curation.
% When relevant data is expensive to obtain it becomes a matter of \textit{exploration} and \textit{prioritization}.
% What data can we collect that makes the best use of the resources we have?

% Algorithmic solutions to efficient data collection go by various names depending on the exact problem setting (e.g. active learning, black-box optimization, reinforcement learning, etc.).
% Most of these algorithms iteratively construct a dataset by training a predictive model on the data already collected and then give the model \textit{agency} to decide which data to collect next.
% If all goes well, a virtuous cycle is created.
% When these algorithms are integrated into systems deployed to consequential settings, we quickly find we cannot allow them to operate unconstrained.
% To operate reliably, we need to control the risk the system incurs while it improves.

% Quantifying risk requires quantifying the uncertainty of our predictions.
% While there are many statistical approaches to uncertainty quantification, most rely on critical distributional assumptions that are difficult to verify or enforce, which limits their practical utility.
% Conformal prediction \citep{vovk2005algorithmic} is a notable exception, as it makes no assumptions about the exact form of the data distribution, but instead relies on more abstract assumptions like exchangeability.
% Unfortunately the data generated by AI agents is not exchangeable, since the data collected at each point in time are statistically dependent on the data that came before.

% \citet{fannjiang2022conformal} proved that conformal prediction could be extended to AI agents for a single round starting from an i.i.d. dataset, building on a key result from \citet{tibshirani2019conformal} for independently (but not identically) distributed data.
% In this work we show that \citet{tibshirani2019conformal}'s results can be reinterpreted to show that conformal prediction can in theory be extended to \textit{any} joint distribution (including ones with sequential dependencies), although the fully general form is not practically feasible ($\mathcal{O}((n+1)!)$) to compute.
% In particular we show that a conformal coverage guarantee for AI agents over multiple rounds is not only theoretically possible, but one of the special cases where the guarantee can be realized in practice.
% To support our theory we provide empirical results on synthetic protein design and active learning tasks which verify that we maintain coverage when baselines from \citet{tibshirani2019conformal} and \citet{fannjiang2022conformal} fail to do so.
