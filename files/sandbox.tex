% intro
This tension between AI safety and capabilities becomes even more prominent and challenging as these systems are increasingly endowed with \textit{agency}---the ability to act autonomously, with consequences to human well-being often at stake.
We focus on three fundamental challenges that emerge when deploying agents for consequential decision-making tasks: (1) Firstly, agents' actions over time may create what we refer to as \textit{feedback-loop shifts}---data distributions that change over time in ways that are dependent on previously observed data---a particularly challenging form of distribution shift where existing uncertainty quantification methods may cease to produce useful outputs.
Prime examples include experimental design, where an agent's data-driven decision about what experiment to run changes the distribution from which it receives new data, and robotics, where an agent chooses new physical environments to explore based on what has already observed.
(2) Secondly, uncertainty quantification methods that do address feedback-loop shifts typically use density ratios to account for the difference between distributions.
Such density ratios often cannot be evaluated exactly and instead must be estimated from data, a problem that is fundamentally ill-posed and difficult to solve reliably, particularly when action spaces are large.
For instance, suppose one is designing a length-$1000$ protein, where each of the $1000$ sites can be any of twenty amino acids.
Existing methods for quantifying uncertainty over a designed sequence's label would require estimating or evaluating the density ratio between the distributions of training and designed sequences across all possible $20^{1000}$ potential sequences.
(3) Lastly, uncertainty quantification alone may not address the goals of the human stakeholders.
Beyond merely quantifying uncertainty, can we leverage it in order to make decisions with better outcomes?



\section{Problem Setup and Background}

\subsection{General Setup: Risk-Constrained Decision Making}

At each time $t=0, 1, ...$, the agent receives an input context (or observation, state, or prompt) $X_t\in \mathcal{X}$ and takes an action $A_t\in\mathcal{A}$.
For example, in the setting of LLMs, $X_t$ could be a text prompt, with $\mathcal{X}=\mathcal{V}^*$ for a vocabulary $\mathcal{V}$, and the LLM's action could be to generate a text response $A_t(X_t)=X_t'$.
After taking action, the agent receives some feedback (e.g., from a human or the environment) in the form of a response (or label) $Y_t'\in \mathcal{Y}$.
Denote $Z_t = (X_t, A_t, Y'_t)$.
At time $t$, the agent uses some procedure to choose a probabilistic policy over action space based on the history, $Z_1, \ldots, Z_{t - 1}$, typically in order to optimize some objective.
In this work, we assume access to such a procedure, and construct a wrapper around it that endows the resulting policies with guarantees of satisfying a domain-appropriate risk constraint.
Specifically, at time $t$, our method produces a policy, $\pi_{\theta_t}$, that satisfies
\begin{align}
    \mathbb{E}_{\pi_{\theta_t}}\big[L(A_t)\big] \leq \alpha
\end{align}
for some loss function $L$ and threshold $\alpha$.
For example, consider $L(A) = \mathbf{1}[A \in \mathcal{F}]$ for some set $\mathcal{F} \subset\mathcal{A}$ of actions that are ``unsafe'' or ``infeasible'' for the domain of interest.
The constraint then means that the policy produces safe or feasible actions with probability at least $1 - \alpha$.

\subsection{Motivating Example: Molecular Design}

For a motivating example throughout this paper, we will often return to the setting of biomolecular design with LLM agents.
In this design setting, let the input space be the space of possible protein sequences, $\mathcal{X}:= \mathcal{V}^*$, where $\mathcal{V}$ is a vocabulary of amino acids.
Given an input ``prompt'' or ``seed'' sequence, $X_t$, the policy generates ``designed'' sequences believed to exhibit desirable values of some property of interest, such as therapeutic efficacy.

% In particular, a biomolecular engineer is generally interested in finding the \textit{best feasible} therapeutic.
The feasible set, $\mathcal{F}$, could denote the unknown set of stable, folded proteins that bind to the target antigen; in other words, the ``prerequisites'' for obtaining a meaningful outcome value, $Y_t'$.
We take the cost function of an ``infeasibility indicator'' as the primary practical constraint we aim to control: 
\begin{align}\label{eq:infeasibility_indicator}
    L_t = l(X_t') := \mathbbm{1}\{X_t'\not\in \mathcal{F}\}.
\end{align}

