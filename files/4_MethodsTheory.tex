


\begin{figure*}[t]
    \centering
    % First subfigure
    \subfloat[$\beta\rightarrow 0$]{%
        \includegraphics[width=0.18\textwidth]{figures/constrained_ex_beta2.225074e-308.pdf}% Replace with your image file
        \label{fig:interpolation_a}%
    }
    % \quad % Adds horizontal space between subfigures
    % Second subfigure
    \quad
    \subfloat[$\beta=10^{-10}$]{%
        \includegraphics[width=0.18\textwidth]{figures/constrained_ex_beta1.000000e-10.pdf}% Replace with your image file
        \label{fig:interpolation_b}%
    }
    \quad
    \subfloat[$\beta=1$]{%
        \includegraphics[width=0.18\textwidth]{figures/constrained_ex_beta1.000000e+00.pdf}% Replace with your image file
        \label{fig:interpolation_c}%
    }
    \quad
    \subfloat[$\beta=100$]{%
        \includegraphics[width=0.18\textwidth]{figures/constrained_ex_beta1.000000e+02.pdf}% Replace with your image file
        \label{fig:interpolation_d}%
    }
    \quad
    \subfloat[$\beta = \infty$]{%
        \includegraphics[width=0.18\textwidth]{figures/constrained_ex_betainf.pdf}% Replace with your image file
        \label{fig:interpolation_e}%
    }
    \caption{Main caption for the entire figure containing subfigures A and B.}
    \label{fig:mainfigure}
\end{figure*}

\section{Methods and Theory}

We first state the only assumptions required by conformal policy control (CPC): access to a safe policy and subsequent unconstrained policies. In Sec. \ref{subsec:interpolating_policies} we introduce an approach to interpolating between arbitrary safe and unconstrained policies; in Sec. \ref{subsec:cpc_selecting_rc_policy} we describe how CPC selects a risk-controlling constrained policy, with oracle and practical guarantees; and in Sec. \ref{subsec:cpc_LLM_methods} we describe how we incorporate CPC into an LLM agent setting for biomolecular design.

\textbf{Assumption 1: An Initial Safe Policy} Assume access to an initial safe policy, $\pi_{\theta_0}$, for which the true risk is controlled at the desired level $\alpha$; that is, assume Eq. \eqref{eq:policy_control} holds for $\pi_{\theta_0}$. %: $\mathbb{E}_{A_t\sim \pi_{\theta_0}}\big[L_t(A_t)\big] \leq \alpha$. 
For example, $\pi_{\theta_0}$ could be trained as a density model of a training set of safe actions, to simply generate similar safe actions without yet optimizing for reward. In protein design, $\pi_{\theta_0}$ might be trained to simply generate feasible sequences without yet extrapolating beyond the training data.

\textbf{Assumption 2: Access to Each Optimistic Policy} Then, at each time $t = 1, 2, ...$, also assume access to the updated ``unconstrained'' model, $\pi_{\theta_t}$, which may be trained to maximize expected reward (as in the ``unconstrained'' part of Eq. \eqref{eq:proxy_goal}). Our methods do not require any assumptions on how $\pi_{\theta_t}$ is trained, although stronger unconstrained performance (with respect to both the constraint and the reward) may translate into better performance of the resulting constrained policy.

% a ``safer'' $\pi_{\theta_t}$ may require less stringent policy control, and if $\pi_{\theta_t}$ performs well with respect to the reward then the resulting constrain






\subsection{Interpolating Between Arbitrary Safe and Unconstrained Policies}
\label{subsec:interpolating_policies}

\textbf{Constraining Policies by Clipping Likelihood Ratios} To constrain each current optimistic policy, $\pi_{\theta_{t}}$, with respect to our safe policy, $\pi_{\theta_{0}}$, we need a general approach to interpolating between the two. There are of course many approaches to interpolating between distributions, but for now consider the following: let us define our constrained policy $\pi_{\theta_{t}}^{(\beta_{t})}$ by ``clipping'' our unconstrained policy $\pi_{\theta_{t}}$ wherever its likelihood ratio reaches or exceeds some bound, $\beta_t > 0$, and then renormalizing. Intuitively, this makes sense---we constrain the chance of actions that are most likely to be taken by the unconstrained policy, relative to their likelihood under the safe policy. 
That is, define $\pi_{\theta_{t}}^{(\beta_{t})}$ piecewise, as
\begin{align}\label{eq:piecewise_constraint_def}
\pi_{\theta_{t}}^{(\beta_{t})}(x) \propto \begin{cases}
    \pi_{\theta_{t}}(x) & \text{if } \pi_{\theta_{t}}(x)/\pi_{\theta_{0}}(x)<\beta_t \\
    \beta_t\cdot \pi_{\theta_{0}}(x) & \text{otherwise.}
\end{cases}
\end{align}
% Notice that in the first condition of Eq. \eqref{eq:piecewise_constraint_def}, $\pi_{\theta_{t}}^{(\beta_{t})}(x)$ is proportional to the optimistic policy, $\pi_{\theta_{t}}(x)$, while in the second condition, $\pi_{\theta_{t}}^{(\beta_{t})}(x)$ is proportional to the scaled safe policy, $\beta_t\cdot \pi_{\theta_{0}}(x)$. 
In Eq. \eqref{eq:piecewise_constraint_def}, notice that the larger the bound $\beta_t$, the more relaxed the constraint; in particular, note that as $\beta_t\rightarrow\infty$, the constrained policy becomes fully unconstrained ($\pi_{\theta_{t}}^{(\beta_{t})}(x)\rightarrow \pi_{\theta_{t}}(x)$), and as $\beta_t\rightarrow 0$, the constrained policy reduces to the safe policy ($\pi_{\theta_{t}}^{(\beta_{t})}(x)\rightarrow \pi_{\theta_{0}}(x)$).


Clipping the likelihood ratio is not a new idea, although it has generally been done in other contexts and for other purposes than as in our work here. For instance, proximal policy optimization (PPO) algorithms \citep{schulman2017proximal} notably use a similar idea by clipping policy gradients for an intuitive objective that can be smoothly optimized. Curiously, however, the constrained policy in Eq. \eqref{eq:piecewise_constraint_def} and our algorithms are perhaps most closely related to the classic accept-reject (AR)-sampling algorithm \citep{von1963various}, as we will describe next.


\textbf{Sampling from the Constrained Policy: Connections to Accept-Reject Sampling} 


% ensures that $\pi_{\theta_{t}}^{(\beta_{t})}$ 





\subsection{Conformal Policy Control: Selecting a Risk-Controlling Policy}
\label{subsec:cpc_selecting_rc_policy}




\subsection{Incorporating Conformal Policy Control into Protein Design with LLMs}
\label{subsec:cpc_LLM_methods}