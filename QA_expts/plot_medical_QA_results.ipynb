{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Plotting Medical QA experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport pickle\nfrom utils import remove_specific_leading_chars\n\nimport matplotlib.pyplot as plt\nimport matplotlib.transforms as mtransforms"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load data (mostly for computing population average FDR)\"\"\"\n",
    "\n",
    "## Loading datasets\n",
    "orig_datasets = {}\n",
    "suffix = \".jsonl\"\n",
    "dataset_dir = \"/home/drewprinster/conformal-safety/data/MedLFQAv2\"  # \"/Users/cherian/Projects/conformal-safety/data/MedLFQAv2\"\n",
    "for path in os.listdir(dataset_dir):\n",
    "    dataset_name = path[: -len(suffix)]\n",
    "    if not path.startswith(\".\"):\n",
    "        with open(os.path.join(dataset_dir, path), \"r\") as fp:\n",
    "            orig_datasets[dataset_name] = [json.loads(line) for line in fp.readlines()]\n",
    "\n",
    "## Dictionary where can search using quesiton and get name of original dataset it came from\n",
    "dataset_lookup = {}\n",
    "for name, data in orig_datasets.items():\n",
    "    for dat in data:\n",
    "        dataset_lookup[dat[\"Question\"]] = name\n",
    "\n",
    "## Getting scores\n",
    "data_path = \"/home/drewprinster/conformal-safety/data\"  # \"/Users/cherian/Projects/conformal-safety/data\"\n",
    "dataset_path = os.path.join(data_path, \"medlfqa_dataset.pkl\")\n",
    "freq_path = os.path.join(data_path, \"medlfqa_frequencies.npz\")\n",
    "logprob_path = os.path.join(data_path, \"medlfqa_logprobs.npz\")\n",
    "selfeval_path = os.path.join(data_path, \"medlfqa_selfevals.npz\")\n",
    "\n",
    "with open(dataset_path, \"rb\") as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "\n",
    "## HARD CODED FIX FOR REDUNDANT PROMPT...which has atomic_facts assigned to the wrong redundancy\n",
    "dataset[1132][\"atomic_facts\"] = dataset[1048][\"atomic_facts\"]\n",
    "\n",
    "\n",
    "frequencies = np.load(freq_path)\n",
    "logprobs = np.load(logprob_path)\n",
    "selfevals = np.load(selfeval_path)\n",
    "\n",
    "drop_prompts = []\n",
    "for k in frequencies:\n",
    "    if frequencies[k].ndim != 1:\n",
    "        drop_prompts.append(k)\n",
    "    elif np.allclose(selfevals[k], -1):\n",
    "        drop_prompts.append(k)\n",
    "    elif k not in logprobs:\n",
    "        drop_prompts.append(k)\n",
    "    elif remove_specific_leading_chars(k).strip() not in dataset_lookup:\n",
    "        drop_prompts.append(k)\n",
    "\n",
    "# drop and match ordering of dataset\n",
    "dataset = [\n",
    "    dat for dat in dataset if dat[\"prompt\"] not in drop_prompts\n",
    "]  ## List where each entry is a dictionary with 'prompt', 'response', and 'atomic_facts'\n",
    "full_dataset = dataset\n",
    "prompts_to_keep = [\n",
    "    dat[\"prompt\"] for dat in dataset\n",
    "]  ## List where each entry is the full prompt\n",
    "names_to_keep = [\n",
    "    p.split(\"about\")[-1].strip()[:-1] for p in prompts_to_keep\n",
    "]  ## List where each entry is an abbreviated prompt for a name\n",
    "\n",
    "## Lists where each entry in the list is an array of scores for subclaims\n",
    "frequencies_arr = [frequencies[p] for p in prompts_to_keep]  ## Frequency scoring\n",
    "selfevals_arr = [selfevals[p] for p in prompts_to_keep]  ## Self-evaluation scoring\n",
    "logprobs_arr = [logprobs[p] for p in prompts_to_keep]  ## Log-probability scoring\n",
    "annotations_arr = [\n",
    "    np.asarray([af[\"is_supported\"] for af in dat[\"atomic_facts\"]]) for dat in dataset\n",
    "]  ## Oracle (annotation) scoring\n",
    "ordinal_arr = [\n",
    "    np.arange(len(f)) for f in frequencies_arr\n",
    "]  ## Ordinal scoring (baseline)\n",
    "\n",
    "\n",
    "## Record maximum subclaim scores\n",
    "\n",
    "# If removing data where all subclaims are true (for harder benchmark)\n",
    "if remove_easy_data:\n",
    "    indicators_all_subclaims_true = []\n",
    "    for i in range(len(dataset)):\n",
    "        indicators_all_subclaims_true.append(\n",
    "            bool(\n",
    "                min(\n",
    "                    [\n",
    "                        dataset[i][\"atomic_facts\"][j][\"is_supported\"]\n",
    "                        for j in range(len(dataset[i][\"atomic_facts\"]))\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    dataset = [d for i, d in enumerate(dataset) if indicators_all_subclaims_true[i]]\n",
    "    full_dataset = dataset\n",
    "    prompts_to_keep = [\n",
    "        dat[\"prompt\"] for dat in dataset\n",
    "    ]  ## List where each entry is the full prompt\n",
    "    names_to_keep = [\n",
    "        p.split(\"about\")[-1].strip()[:-1] for p in prompts_to_keep\n",
    "    ]  ## List where each entry is an abbreviated prompt for a name\n",
    "\n",
    "    frequencies_arr = [\n",
    "        d for i, d in enumerate(frequencies_arr) if indicators_all_subclaims_true[i]\n",
    "    ]  ## Frequency scoring\n",
    "    selfevals_arr = [\n",
    "        d for i, d in enumerate(selfevals_arr) if indicators_all_subclaims_true[i]\n",
    "    ]  ## Self-evaluation scoring\n",
    "    logprobs_arr = [\n",
    "        d for i, d in enumerate(logprobs_arr) if indicators_all_subclaims_true[i]\n",
    "    ]  ## Log-probability scoring\n",
    "    annotations_arr = [\n",
    "        d for i, d in enumerate(annotations_arr) if indicators_all_subclaims_true[i]\n",
    "    ]  ## Oracle (annotation) scoring\n",
    "    ordinal_arr = [\n",
    "        d for i, d in enumerate(ordinal_arr) if indicators_all_subclaims_true[i]\n",
    "    ]  ## Ordinal scoring (baseline)\n",
    "\n",
    "\n",
    "print(len(frequencies_arr), len(selfevals_arr), len(dataset), len(annotations_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLUE = \"#2166ac\"  # β̂ vertical line\n",
    "TEAL = \"#5ab4ac\"  # Calibration data (complements blue/red)\n",
    "RED = \"#d73027\"  # Test point (conservative/warning)\n",
    "\n",
    "\n",
    "## Calculate population average FDR for unfactual subclaims\n",
    "losses = np.zeros(len(annotations_arr))\n",
    "for i, a in enumerate(annotations_arr):\n",
    "    losses[i] = np.mean(1 - a)\n",
    "pop_avg_frac_unfactual = losses.mean()\n",
    "\n",
    "\n",
    "## Plotting from loading\n",
    "score_names = [\n",
    "    \"logprobs\",\n",
    "    \"selfevals\",\n",
    "    \"frequency\",\n",
    "]  # \"frequency\", \"selfevals\", \"logprobs\"\n",
    "n_trials = 10\n",
    "loss_name = \"loss_factuality_fraction\"  # \"loss_factuality_fraction\"\n",
    "method_names = [\n",
    "    \"gcrc\",\n",
    "    \"ltt\",\n",
    "    \"monotized_losses_crc\",\n",
    "]  # \"standard_crc\", \"gcrc\", \"monotized_losses_crc\", \"monotized_losses_crc\", \"ltt\",\n",
    "ngrid = 200\n",
    "\n",
    "for s_i, score_name in enumerate(score_names):  # frequencies_arr, selfevals_arr,\n",
    "    results_df = pd.read_csv(\n",
    "        f\"/home/drewprinster/conformal-safety/notebooks/{loss_name}Loss_{score_name}Scoring_{n_trials}trials_{ngrid}ngrid_v2.csv\"\n",
    "    )\n",
    "\n",
    "    # display(results_df)\n",
    "    # print(results_df.columns)\n",
    "\n",
    "    method_name_map = {\n",
    "        \"gcrc\": \"GCRC (proposed)\",\n",
    "        \"monotized_losses_crc\": \"Monotized-losses CRC (Mohri & Hashimoto, 2024; Angelopoulos et al., 2024)\",\n",
    "        \"standard_crc\": \"standard CRC (Angelopoulos, et al., 2024)\",\n",
    "        \"ltt\": \"LTT (Angelopoulos, et al., 2025)\",\n",
    "    }\n",
    "\n",
    "    colors_dict = {\n",
    "        \"gcrc\": BLUE,\n",
    "        \"monotized_losses_crc\": \"C5\",\n",
    "        \"ltt\": \"C2\",\n",
    "        \"standard_crc\": \"C1\",\n",
    "    }\n",
    "    markers_dict = {\n",
    "        \"gcrc\": \"o\",\n",
    "        \"monotized_losses_crc\": \"s\",\n",
    "        \"ltt\": \"^\",\n",
    "        \"standard_crc\": \"X\",\n",
    "    }\n",
    "\n",
    "    axlabelsize = 20\n",
    "    ticklabelsize = 18\n",
    "    plt.rc(\"axes.spines\", top=False, right=False)\n",
    "    markersize = 7\n",
    "    linewidth = 2\n",
    "\n",
    "    for metric in [\"risk\", \"claims\"]:\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        alphas = results_df[\"alphas\"]\n",
    "\n",
    "        # metric_dict = eval(f'{metric}_dict')\n",
    "        if metric == \"risk\":\n",
    "            ax.plot(\n",
    "                alphas,\n",
    "                alphas,\n",
    "                linestyle=\"--\",\n",
    "                color=\"black\",\n",
    "                label=\"Target Risk Upper Bound\",\n",
    "                linewidth=linewidth,\n",
    "            )\n",
    "            ax.axhline(\n",
    "                pop_avg_frac_unfactual,\n",
    "                color=\"gray\",\n",
    "                label=\"Population Avg. Risk\",\n",
    "                linestyle=\":\",\n",
    "                linewidth=linewidth,\n",
    "            )\n",
    "            ax.set_ylabel(\n",
    "                r\"FDR (False Claim Rate) [$\\leftarrow$]\", fontsize=axlabelsize\n",
    "            )\n",
    "        else:\n",
    "            ax.set_ylabel(\n",
    "                r\"Recall (for True Claims) [$\\rightarrow$]\", fontsize=axlabelsize\n",
    "            )\n",
    "\n",
    "        for m, method_name in enumerate(method_names):\n",
    "            metric_mean = results_df[f\"{method_name}_{metric}_mean\"]\n",
    "            metric_std = results_df[f\"{method_name}_{metric}_std\"]\n",
    "\n",
    "            ax.fill_between(\n",
    "                alphas,\n",
    "                metric_mean - metric_std / np.sqrt(n_trials),\n",
    "                metric_mean + metric_std / np.sqrt(n_trials),\n",
    "                color=colors_dict[method_name],\n",
    "                alpha=0.5,\n",
    "            )\n",
    "            ax.plot(\n",
    "                alphas,\n",
    "                metric_mean,\n",
    "                marker=markers_dict[method_name],\n",
    "                label=method_name_map[method_name],\n",
    "                color=colors_dict[method_name],\n",
    "                markersize=markersize,\n",
    "                linewidth=linewidth,\n",
    "            )\n",
    "\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=ticklabelsize)\n",
    "        ax.set_xlabel(r\"Target Risk Upper Bound ($\\alpha$)\", fontsize=axlabelsize)\n",
    "        ax.set_xlim([0.0, 0.087])\n",
    "        if metric == \"risk\":\n",
    "            ax.set_ylim([0.0, 0.087])\n",
    "        else:\n",
    "            ax.set_ylim([0.0, 1.02])\n",
    "        fig.savefig(\n",
    "            f\"{loss_name}Loss_{score_names[s_i]}Scoring_{metric}_{n_trials}trials_{ngrid}ngrid.pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=300,\n",
    "        )\n",
    "\n",
    "        if metric == \"risk\":\n",
    "            ## Save legend\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            print(labels)\n",
    "            leg1 = ax.legend(\n",
    "                handles[0:3],\n",
    "                labels[0:3],\n",
    "                loc=\"lower left\",\n",
    "                bbox_to_anchor=(0.5, 1.05),\n",
    "                ncols=3,\n",
    "                frameon=False,\n",
    "            )\n",
    "            ax.add_artist(leg1)\n",
    "            legend_extent1 = leg1.get_tightbbox(fig.canvas.get_renderer()).transformed(\n",
    "                fig.dpi_scale_trans.inverted()\n",
    "            )\n",
    "            leg1.set_in_layout(False)\n",
    "            # fig.savefig(f'legend1_{score_name}_{metric}_v2.pdf', bbox_inches=legend_extent1, dpi=300)\n",
    "\n",
    "            leg2 = ax.legend(\n",
    "                handles[3:],\n",
    "                labels[3:],\n",
    "                loc=\"lower left\",\n",
    "                bbox_to_anchor=(0.5, 1.0),\n",
    "                ncols=2,\n",
    "                frameon=False,\n",
    "            )\n",
    "            # ax.add_artist(leg2)\n",
    "            # leg1.set_in_layout(False)\n",
    "            leg2.set_in_layout(False)\n",
    "            legend_extent2 = leg2.get_tightbbox(fig.canvas.get_renderer()).transformed(\n",
    "                fig.dpi_scale_trans.inverted()\n",
    "            )\n",
    "            union_bbox = mtransforms.Bbox.union([legend_extent1, legend_extent2])\n",
    "            print(union_bbox.x0, union_bbox.y0, union_bbox.x1, union_bbox.y1)\n",
    "\n",
    "            # Create a Rectangle patch\n",
    "            # Set `facecolor='none'` to make the inside transparent\n",
    "            # Set `edgecolor` and `linewidth` to customize the border\n",
    "            # rect = patches.Rectangle((union_bbox.x0, union_bbox.y0), union_bbox.x1, union_bbox.y1, linewidth=2, edgecolor='black', facecolor='none')\n",
    "\n",
    "            # Add the patch to the axes\n",
    "            # ax.add_patch(rect)\n",
    "\n",
    "            fig.savefig(\n",
    "                f\"legend_{score_name}_{metric}.pdf\", bbox_inches=union_bbox, dpi=300\n",
    "            )\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    # results_df.to_csv(f'{loss_name}Loss_{score_names[s_i]}Scoring_{metric}_{n_trials}trials_notebook.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conformal-safety",
   "language": "python",
   "name": "conformal-safety"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}