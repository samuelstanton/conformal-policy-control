
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{SCRATCH space for `Conformal Decision Theory for AI Agents'}

\author{
    Drew Prinster \& Samuel Stanton \\
    Genentech \\
    1 DNA Way, San Francisco, CA \\
    \texttt{\{prinster.drew,stanton.samuel\}@gene.com}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\drew}[1]{\textcolor{blue}{[Dr: #1]}}


% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle



\subsection{Algorithm sketch (mixture approach)}

A sketch of the proposed algorithm for protein design, that modifies the Llome algorithm described in \citet{chen2025generalists} to incorporate conformal policy control.

\begin{enumerate}
    \item \textbf{Initialization:} At time $t=0$, receive a pre-trained LLM $\pi_{\theta_0}$. In particular, we assume that $\pi_{\theta_0}$ was trained on some initial dataset of protein sequences, $\mathcal{D}_0^{\text{train}}:=\{x_i^{\text{train}}\}_{i=1}^{n_0^{\text{train}}}$, by some autoregressive next-token-prediction algorithm $\mathcal{A}$, that is, $\mathcal{A}(\mathcal{D}_0^{\text{train}})\rightarrow \pi_{\theta_0}$. 
    
    We assume that $\pi_{\theta_0}$ satisfies the initial safety criterion, that is:
    \begin{align*}
        \mathbb{E}_{x_0'\sim \pi_{\theta_0}(\cdot)}\big[\mathbbm{1}\{x_0'\not\in \mathcal{F}\}\big]\leq \alpha.
    \end{align*}
    In particular, in our implementation, $\pi_{\theta_0}$ will be trained autoregressively on a dataset of protein sequences collected by the genetic algorithm. We then use this pre-trained LLM to sample an initial calibration set of size  $n_0^{\text{cal}}$, that is, 
    \begin{align*}
        \{x_i^{\text{cal}}\}_{i=1}^{n_0^{\text{cal}}} \sim \pi_{\theta_0}.
    \end{align*}
    This initial calibration dataset, which we may denote $\mathcal{D}_0^{\text{cal}}:=\{x_i\}_{i=1}^{n_0^{\text{cal}}}$, can also be used to empirically ensure that $\pi_{\theta_0}$ is indeed safe, by verifying that $\sum_{i=1}^{n_0^{\text{cal}}}\frac{1}{n_0^{\text{cal}}}\mathbbm{1}\{x_i^{\text{cal}}\not\in\mathcal{F}\}\leq \alpha$.

    We take $\mathcal{D}_0^{\text{train}}$ and reformat it into matched pairs, as in propen \citep{tagasovska2024implicitly}
    
    % Denote this initial calibration dataset as $\mathcal{D}_0:=\{x_i\}_{i=1}^{n_0}$. If $\pi_{\theta_0}$ is known to be safe with respect to the user-specified safety criterion, then proceed to Step 2. If not, then $\pi_{\theta_0}$ can be tuned with 
    % \item \textbf{Policy improvement:} At each time $t$, we obtain an updated ``unconstrained'' policy $\pi_{\theta_t}(x'_t\mid x_t)$ for sampling design sequences $x'_t$ based on a seed sequence $x_t$ (e.g., ``policy improvement/outer loop'' in \citet{chen2025generalists}). 
    
    \item \textbf{Policy control and execution:} 
    \begin{itemize}
        \item \textit{Prompting:} Select input prompt(s) $x_t$ deterministically as the top-scoring historical sequences in past training data (which must be disjoint from calibration data). (Potentially this must include the seed sequence used at the last iteration.)
        \item \textit{Conformal policy control:} From the Policy Improvement Outer Loop and the Prompt, we obtain an ``unconstrained'' policy $\pi_{\theta_t}(\cdot \mid x_t)$. However, the risk constraint might not hold for  $\pi_{\theta_t}$; to obtain a policy where the risk constraint is guaranteed to hold, we instead consider a \textit{mixture} of the most recent policy and the current policy: $\pi_{\theta_t}^{\beta_t}(\cdot \mid x_t) := \beta_t\cdot \pi_{\theta_{t-1}}(\cdot \mid x_t) + (1-\beta_t)\cdot\pi_{\theta_{t}}(\cdot \mid x_t)$, where $\beta_t\in[0,1]$ represents a level of conservativeness. Or, for ease of implementation, we may focus on interpolating with the initial prior policy:  $\pi_{\theta_t}^{\beta_t}(\cdot \mid x_t) := \beta_t\cdot \pi_{\theta_{0}}(\cdot) + (1-\beta_t)\cdot\pi_{\theta_{t}}(\cdot \mid x_t)$. In either case, $\beta_t=0$ corresponds to using the using the unconstrained policy, and increasing $\beta_t\rightarrow 1$ corresponds to returning to the previously-known ``safe'' policy. 

        Then, with the ``infeasibility loss'' $L_t:=\mathbbm{1}\{x_t'\not\in \mathcal{F}\}$ and with CP weights  $\tilde{w}_i(\beta_t)\propto \pi_{\theta_{t}}(x_i \mid x_t)$ for all $x_i\in \mathcal{D}_{\text{cal}}$, use conformal policy control to find\footnote{Note: In Eq. \eqref{eq:beta_hat_t} we may be able to replace $\max_{j\in \mathcal{D}_{\text{pool}}}\tilde{w}_{j}(\beta_j)$ with $\mathbb{E}[\tilde{w}_{j}(\beta_j)] = \sum_{j\in \mathcal{D}_{\text{pool}}}\tilde{w}_{j}(\beta_j)^2$, putting a pin in this.}
        \begin{align}\label{eq:beta_hat_t}
            \hat{\beta}_t=\inf \Big\{ \beta_t : \sum_{i\in\mathcal{D}_{\text{cal}}}\tilde{w}_i(\beta_t)\cdot\mathbbm{1}\{x'_{i}\not\in\mathcal{F}\}+\max_{j\in \mathcal{D}_{\text{pool}}}\tilde{w}_{j}(\beta_j) \leq \alpha\Big \},
        \end{align}
        which will achieve the desired safety guarantee
        \begin{align*}
        \mathbb{E}_{x'_{t}\sim \pi_{\theta_{t}}^{(\hat{\beta}_{t})}}[\mathbbm{1}\{x'_{t}\not\in\mathcal{F}\}]\leq \alpha.
        \end{align*}

        \item \textit{Policy execution:} Sample design sequence(s) with the controlled policy. That is, 
        \begin{align}
            x_t' \sim \pi_{\theta_{t}}^{(\hat{\beta}_{t})}.
        \end{align}
        % \begin{align}\label{eq:crc_infeasible_loss}
        %     \mathbb{E}_{x'_t\sim \pi_{\theta_t}^{\beta_t}}[\mathbbm{1}\{x'_t\not\in\mathcal{F}\}\mid L_{<t}]\leq \alpha
        % \end{align}
        % \item Using the conformal risk control procedure, let $L_t:=\mathbbm{1}\{x_t'\not\in \mathcal{F}\}$. Then, assuming (by induction) that the previous policy was also calibrated to control the risk, i.e., 
        % \begin{align*}
        % \mathbb{E}_{x'_{t-1}\sim \pi_{\theta_{t-1}}^{\beta_{t-1}}}[\mathbbm{1}\{x'_{t-1}\not\in\mathcal{F}\}\mid L_{<t-1}]\leq \alpha,
        % \end{align*}
        % it is reasonable to assume that the risk is nonincreasing in $\beta_t$, as $\beta_t=0$ is the updated (aggressive) policy, and $\beta_t=1$ corresponds to the known safe policy (by construction, from the previous time). So, we can run weighted CRC to find the smallest $\beta_t$ that controls the risk of infeasibility under the current policy,
        % \begin{align}
        %     \hat{\beta}_t=\inf \Big\{ \beta_t : \sum_{i\in\mathcal{D}_{\text{cal}}}\tilde{w}_i(\beta_t)\cdot\mathbbm{1}\{x'_{i}\not\in\mathcal{F}\}+\tilde{w}_{t}(\beta_t) \leq \alpha\Big \},
        % \end{align}
        % where $\tilde{w}_i(\beta_t)$ is the normalized CP weight for the policy $\pi_{\theta_t}^{\beta_t}(\cdot \mid x_t)$.
        
        % \drew{Possibility for some sort of relaxation in CRC monotonicity condition, where maybe the loss function itself does not need to be nonincreasing in $\beta$, but maybe only the \textit{expected} loss/distribution of loss may need to be decreasing?}
        
        % \item Use (seeded) policy to sample\footnote{Or, may sample$\{x_{t,1}', ..., x_{t,k}'\} \sim \pi_{\theta_t}(\cdot \mid x_t)$ and then select from them, but this introduces further complexity that will ignore for now.} $x_t'\sim \pi_{\theta_t}(\cdot \mid x_t)$
    \end{itemize}
\end{enumerate}



\textbf{Problem formulation:} \drew{Given that we're targeting ICLR/AIStats, I think it will be a good idea to have a general formulation of the problem that is not limited to protein design, but then to make sure that we cover protein design as a main instance. This should allow us to develop novelty in the direction of online conformal risk control (CRC), but not limiting ourselves fully to whether CRC is really the types of risks we want to control.}

\drew{Maybe would be good to focus on controlling loss on actions, as we did in our proposal: $\mathbb{E}[l(A_{n+1}, Y_{n+1})]$; however, can consider two cases, one where the actions are a function of the point prediction (corresponding to expectation-maximizing agents): $\mathbb{E}[l(A_{n+1}(\hat{f}(x)), Y_{n+1})]$; and a second where the actions are a function of a set prediction (corresponding to risk-averse agents): $\mathbb{E}[l(A_{n+1}(\hat{C}(x)), Y_{n+1})]$.}

\bibliography{references}
\bibliographystyle{iclr2025_conference}


\end{document}
